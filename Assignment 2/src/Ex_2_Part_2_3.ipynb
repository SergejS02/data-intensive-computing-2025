{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f400a1-9cf2-4303-9cea-6cf1d2806180",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a73033a-7a16-47d5-82bf-1c3a0a551557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/11 00:07:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#open spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"A2-Part2-Pipeline\").getOrCreate()\n",
    "\n",
    "#load reviews and stopwords\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    RAW_PATH = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "else:\n",
    "    RAW_PATH = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "stopwordsPath = \"Exercise_1/stopwords.txt\"\n",
    "#define structure of json for faster reading\n",
    "from pyspark.sql import types as T\n",
    "review_schema = T.StructType([\n",
    "     T.StructField(\"reviewerID\",      T.StringType(),  True),\n",
    "     T.StructField(\"asin\",            T.StringType(),  True),\n",
    "     T.StructField(\"reviewerName\",    T.StringType(),  True),\n",
    "     T.StructField(\"helpful\",         T.ArrayType(T.IntegerType()), True),\n",
    "     T.StructField(\"reviewText\",      T.StringType(),  True),\n",
    "     T.StructField(\"overall\",         T.FloatType(),   True),\n",
    "     T.StructField(\"summary\",         T.StringType(),  True),\n",
    "     T.StructField(\"unixReviewTime\",  T.LongType(),    True),\n",
    "     T.StructField(\"reviewTime\",      T.StringType(),  True),\n",
    "     T.StructField(\"category\",        T.StringType(),  True),\n",
    " ])\n",
    "#read and select category and review\n",
    "df = (\n",
    "    spark.read\n",
    "         .schema(review_schema)\n",
    "         .json(RAW_PATH)\n",
    "         .selectExpr(\"reviewText AS text\",\n",
    "                     \"category\")\n",
    "    .na.drop(subset=[\"text\", \"category\"])\n",
    ")\n",
    "\n",
    "# reading the stopwords\n",
    "stopwords = spark.sparkContext.textFile(stopwordsPath).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54af8e1-0dd3-4ae0-a6ea-b620812dafaf",
   "metadata": {},
   "source": [
    "Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03933214-ef07-42a7-a80e-0759ec54b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer,\n",
    "    StopWordsRemover,\n",
    "    CountVectorizer,\n",
    "    IDF,\n",
    "    ChiSqSelector,\n",
    "    StringIndexer\n",
    ")\n",
    "\n",
    "# 1 Tokenisation and lower-casing via RegexTokenizer\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"text\",\n",
    "    outputCol=\"tokens\",\n",
    "    pattern=r\"\"\"[ \\t0-9()\\[\\]{}.!?,;:+=\\-_\"'`~#@&*%â‚¬$Â§\\\\/]+\"\"\",  # delimiters\n",
    "    gaps=True,                # pattern defines the split points\n",
    "    toLowercase=True,\n",
    ")\n",
    "\n",
    "# 2 Stopword removal\n",
    "stopper = StopWordsRemover(inputCol=\"tokens\",stopWords = stopwords, outputCol=\"clean_tokens\")\n",
    "\n",
    "\n",
    "# 3 Vectorizing\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"clean_tokens\",\n",
    "    outputCol=\"tf\",\n",
    "    minDF=2,\n",
    "    vocabSize=50_000, \n",
    ")\n",
    "\n",
    "# 4 IDF weighting\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "\n",
    "# 5 encode the category column from string to int\n",
    "encoder = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "# 6 select top 2000 terms by chiÂ²\n",
    "selector = ChiSqSelector(\n",
    "    numTopFeatures=2000,\n",
    "    featuresCol=\"tfidf\",\n",
    "    outputCol=\"chi2_features\",\n",
    "    labelCol=\"label\",\n",
    ")\n",
    "\n",
    "# 7 Pipeline assembly\n",
    "pipeline = Pipeline(stages=[tokenizer, stopper, cv, idf, encoder, selector])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594a4d69-244a-4402-b584-c5c7f00213e3",
   "metadata": {},
   "source": [
    "Fit pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63afc1c-cc8c-4d1b-a256-f9fc003c8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"128\")\n",
    "df.persist() # persisting intermediate output, better for large datasets\n",
    "model = pipeline.fit(df)\n",
    "df.unpersist()\n",
    "\n",
    "# Extract vocabulary & selected indices\n",
    "vocab = model.stages[2].vocabulary                        # get vocabulary from Vecorizer\n",
    "selected = model.stages[-1].selectedFeatures             # index of term after selector\n",
    "\n",
    "selected_terms = [vocab[i] for i in selected]            # map indices to terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c442d23-66d0-438b-b187-297331c4e35b",
   "metadata": {},
   "source": [
    "Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e30e7ae-6cac-4a34-be9b-093766dbd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2000 terms to /home/e12412694/DIC/Exercise_2/output_ds.txt\n"
     ]
    }
   ],
   "source": [
    "# Saves top 2000 terms to output_ds.txt (one term per line)\n",
    "import pathlib, os, codecs\n",
    "\n",
    "out_file = pathlib.Path(\"output_ds.txt\")\n",
    "out_file.write_text(\"\\n\".join(selected_terms), encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(selected_terms)} terms to {out_file.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "071a1f36-be16-4fab-9cfd-bee01ac49f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Optional: automatic comparison with Assignment 1\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# old_terms = pathlib.Path(\"assignment1_terms.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "# old_set, new_set = set(old_terms), set(selected_terms)\n",
    "# print(\"\\nðŸ”¹  Terms kept in *both* assignments:\", len(old_set & new_set))\n",
    "# print(\"ðŸ”¸  Terms only in Assignment 1:\",      len(old_set - new_set))\n",
    "# print(\"ðŸ”¸  Terms only in Spark pipeline:\",    len(new_set - old_set))\n",
    "# (You can also diff the two files directly with any text-diff tool.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9d159b-bed8-4bc7-b7ee-a7902483fd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms in BOTH assignments: 2000\n",
      "Terms ONLY in Assignment 1: 97017\n",
      "Terms ONLY in Spark pipeline (Assignment 2): 0\n",
      "Overlap: 100.00% of Spark-selected terms are also in Assignment 1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read the file and get the last line (merged vocabulary)\n",
    "lines = pathlib.Path(\"../Exercise_1/src/output_dev.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "merged_vocab_line = lines[-1]  # This is the merged vocab line\n",
    "\n",
    "# Step 2: Split merged vocab into terms\n",
    "old_terms = merged_vocab_line.strip().split()\n",
    "old_set = set(old_terms)\n",
    "\n",
    "# Step 3: Load selected terms from Spark pipeline (output_ds.txt)\n",
    "new_terms = pathlib.Path(\"output_ds.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "new_set = set(term.strip() for term in new_terms)\n",
    "\n",
    "# Step 4: Compare sets\n",
    "common_terms = old_set & new_set\n",
    "only_in_old = old_set - new_set\n",
    "only_in_new = new_set - old_set\n",
    "\n",
    "# Step 5: Print results\n",
    "print(f\"Terms in BOTH assignments: {len(common_terms)}\")\n",
    "print(f\"Terms ONLY in Assignment 1: {len(only_in_old)}\")\n",
    "print(f\"Terms ONLY in Spark pipeline (Assignment 2): {len(only_in_new)}\")\n",
    "print(f\"Overlap: {len(common_terms) / len(new_set) * 100:.2f}% of Spark-selected terms are also in Assignment 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a22937-d901-4d24-80e1-bd48d09ea236",
   "metadata": {},
   "source": [
    "# Part 3 \n",
    "// Not working yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59f7bd03-d489-4209-853b-bf27a441d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_3 = model.transform(df).select(\"label\",\"chi2_features\").toDF(\"label\",\"chi2_features\")\n",
    "#display(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b4f073-a26d-4413-8c2e-66f29451a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.classification import LinearSVC,  OneVsRest\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")   # WARN-Meldungen verschwinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f076070d-2827-4e25-a701-ef7d5b862e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 normalization of the selected features\n",
    "normalizer = Normalizer(inputCol=\"chi2_features\", outputCol=\"norm_features\", p=2.0)\n",
    "\n",
    "# Binary SVM\n",
    "svm = LinearSVC(\n",
    "    featuresCol=\"norm_features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=2, # for testing\n",
    "    regParam=0.1,\n",
    "    standardization=True\n",
    ")\n",
    "\n",
    "# One-vs-Rest strategy for multi-class SVM\n",
    "ovr = OneVsRest(\n",
    "    classifier=svm,\n",
    "    featuresCol=\"norm_features\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n",
    "# Update pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stopper,\n",
    "    cv,\n",
    "    idf,\n",
    "    encoder,\n",
    "    selector,\n",
    "    normalizer,\n",
    "    ovr\n",
    "])\n",
    "\n",
    "# Split data\n",
    "train, val, test = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Init evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a59b014-e191-4a90-a0ee-e5c60a0ad60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score: 0.5608\n"
     ]
    }
   ],
   "source": [
    "# Train pipeline\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(f\"Test F1 score: {f1:.4f}\")\n",
    "\n",
    "# took like 15mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a78b6e-a573-4885-813d-2114c3240b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(svm.maxIter, [5, 10]) \\\n",
    "    .addGrid(svm.standardization, [True, False]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train cv\n",
    "cv_model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef35927-1eb7-4af2-ac3c-c6a87ab0baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = cv_model.avgMetrics\n",
    "param_maps = cv.getEstimatorParamMaps()\n",
    "\n",
    "# Check all models\n",
    "print(\"Cross-Validation Results:\")\n",
    "for i, (params, score) in enumerate(zip(param_maps, avg_metrics)):\n",
    "    print(f\"Model {i+1}: F1 = {score:.4f}, Params: {params}\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = cv_model.bestModel\n",
    "print(\"Best Model Params:\")\n",
    "for stage in best_model.stages:\n",
    "    if hasattr(stage, 'extractParamMap'):\n",
    "        print(stage.__class__.__name__, stage.extractParamMap())\n",
    "\n",
    "# Evaluate best model on the test set\n",
    "predictions_test = best_model.transform(test)\n",
    "f1_test = evaluator.evaluate(predictions_test)\n",
    "print(f\"Best Model F1 Score on Test Set: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d049ec62-ca4f-4979-ac56-45a89f178faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "832198eb-1237-49e1-a7bd-7e52975ba400",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m lsvc \u001b[38;5;241m=\u001b[39m LinearSVC(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxIter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     15\u001b[0m ovr \u001b[38;5;241m=\u001b[39m OneVsRest(classifier\u001b[38;5;241m=\u001b[39mlsvc, featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m ovr_model \u001b[38;5;241m=\u001b[39m \u001b[43movr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m param_grid_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier__regParam\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m],\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier__standardization\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m],\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier__maxIter\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m]\n\u001b[1;32m     21\u001b[0m }\n\u001b[1;32m     22\u001b[0m param_grid_builder \u001b[38;5;241m=\u001b[39m ParamGridBuilder()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/classification.py:3586\u001b[0m, in \u001b[0;36mOneVsRest._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   3582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classifier\u001b[38;5;241m.\u001b[39mfit(trainingDataset, paramMap)\n\u001b[1;32m   3584\u001b[0m pool \u001b[38;5;241m=\u001b[39m ThreadPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetParallelism(), numClasses))\n\u001b[0;32m-> 3586\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minheritable_thread_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainSingleClass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumClasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handlePersistence:\n\u001b[1;32m   3589\u001b[0m     multiclassLabeled\u001b[38;5;241m.\u001b[39munpersist()\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib64/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prev ver\n",
    "transformedData = model.transform(df)df=transformedData\n",
    "df=df.sample(fraction=0.01, seed=4242)\n",
    "df2=df.select(\"label\", \"chi2_features\").toDF(\"label\", \"selected\")\n",
    "normalizer = Normalizer().setInputCol(\"selected\").setOutputCol(\"normalized\").setP(2.0)\n",
    "df_norm =normalizer.transform(df2)\n",
    "df3=df_norm.select(\"label\", \"normalized\").toDF(\"label\", \"normalized\")\n",
    "train,val, test = df3.randomSplit([0.7,0.15, 0.15], seed = 4242)\n",
    "lsvc = LinearSVC(featuresCol=\"normalized\", labelCol=\"label\", maxIter=10)\n",
    "ovr = OneVsRest(classifier=lsvc, featuresCol=\"normalized\", labelCol=\"label\")\n",
    "ovr_model = ovr.fit(train)\n",
    "param_grid_dict = {\n",
    "    \"classifier__regParam\": [0.001, 0.01, 0.1],\n",
    "    \"classifier__standardization\": [True, False],\n",
    "    \"classifier__maxIter\": [10, 8]\n",
    "}\n",
    "param_grid_builder = ParamGridBuilder()\n",
    "for param, values in param_grid_dict.items():   \n",
    "    param_grid_builder = param_grid_builder.addGrid(getattr(lsvc, param.split(\"__\")[1]), values)\n",
    "\n",
    "# Building the parameter grid using the added grids\n",
    "param_grid = param_grid_builder.build()\n",
    "# The F1 score is the harmonic mean of precision and recall\n",
    "evaluator=MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "cv=CrossValidator(estimator=ovr, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=2)\n",
    "val.groupBy(\"label\").count().show()\n",
    "cv_model=cv.fit(val)\n",
    "best_model=cv_model.bestModel\n",
    "ovr_predictions_test = ovr_model.transform(test)\n",
    "ovr_f1_score = evaluator.evaluate(ovr_predictions_test)\n",
    "print(f\"OVR F1 Score: {ovr_f1_score}\")\n",
    "best_model_predictions_test = best_model.transform(test)\n",
    "best_model_f1_score = evaluator.evaluate(best_model_predictions_test)\n",
    "print(f\"Best Model F1 Score: {best_model_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7462a-b782-4562-ac33-79ba8a83d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1.  Einmalige Feature-Extraktion  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")   # WARN-Meldungen verschwinden\n",
    "features_df = (\n",
    "    model            # CountVectorizer + IDF + ChiÂ² (fitted)\n",
    "    .transform(df)         # âžœ neue Spalten: tf, tfidf, chi2_features\n",
    "    .select(\"chi2_features\", \"label\")        # + evtl. andere Meta-Spalten\n",
    "    .cache()\n",
    ")\n",
    "print(features_df.count(), \"Dokumente nach Feature-Extraktion\")\n",
    "\n",
    "# â”€â”€ 2.  Light-Pipeline fÃ¼r den Klassifikator  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Normalizer, StringIndexer\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "indexer   = StringIndexer(inputCol=\"label\",  outputCol=\"label_idx\")\n",
    "norm      = Normalizer(inputCol=\"chi2_features\", outputCol=\"features\", p=2.0)\n",
    "svm_bin   = LinearSVC(featuresCol=\"features\", labelCol=\"label_idx\", maxIter=10)\n",
    "ovr       = OneVsRest(classifier=svm_bin, featuresCol=\"features\", labelCol=\"label_idx\")\n",
    "\n",
    "pipe3 = Pipeline(stages=[indexer, norm, ovr])\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "      .addGrid(svm_bin.regParam,        [0.01])\n",
    "      .addGrid(svm_bin.standardization, [True])\n",
    "      .addGrid(svm_bin.maxIter,         [50, 200])\n",
    "      .build()\n",
    ")\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=pipe3,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=MulticlassClassificationEvaluator(labelCol=\"label_idx\", metricName=\"f1\"),\n",
    "    trainRatio=0.8,\n",
    "    seed=42,\n",
    "    parallelism=4,\n",
    ")\n",
    "\n",
    "train_df, test_df = features_df.randomSplit([0.8, 0.2], seed=42)\n",
    "best_model = tvs.fit(train_df).bestModel\n",
    "print(\"F1 on test:\",\n",
    "      MulticlassClassificationEvaluator(labelCol=\"label_idx\", metricName=\"f1\")\n",
    "      .evaluate(best_model.transform(test_df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5697adc-aa95-4ad0-9195-473f311bdeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
