{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497c3fe5-37f4-4ea6-a20e-4436fdaa4e9f",
   "metadata": {},
   "source": [
    "Intitializing Setup and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739efa21-7626-4780-811d-c937ed8a62cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://lbdmg01.datalab.novalocal:9999/proxy/application_1745308556449_5514\n",
       "SparkContext available as 'sc' (version = 3.3.4, master = yarn, app id = application_1745308556449_5514)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime Parsing: 8918.06 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import scala.util.parsing.json.JSON\n",
       "import scala.util.matching.Regex\n",
       "start: Long = 12150565117071384\n",
       "total: Long = 12150565117074115\n",
       "inputPath: String = hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\n",
       "stopwordPath: String = Exercise_1/assets/stopwords.txt\n",
       "TOP_K: Int = 75\n",
       "DELIMS: String = [()\\[\\]{}.!?,;:+=\\-_\"'`~#@&*%€§\\\\/0-9]+\n",
       "stopwords: scala.collection.immutable.Set[String] = Set(serious, latterly, absorbs, looks, particularly, used, e, printer, down, regarding, entirely, regardless, moreover, please, read, ourselves, able, behind, for, despite, s, maybe, viz, further, corresponding, x, any, wherein, across, name, allows, this, instead, in, taste, ought, myself, have, your, off, once, are, is, mon, his, oh, why, rd, knows, bul...\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import scala.util.parsing.json.JSON\n",
    "import scala.util.matching.Regex\n",
    "\n",
    "var start = System.nanoTime()\n",
    "var total = System.nanoTime()\n",
    "\n",
    "//paths\n",
    "val inputPath = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "//val inputPath = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "val stopwordPath = \"Exercise_1/assets/stopwords.txt\"\n",
    "\n",
    "val TOP_K = 75\n",
    "val DELIMS = \"[()\\\\[\\\\]{}.!?,;:+=\\\\-_\\\"'`~#@&*%€§\\\\\\\\/0-9]+\"\n",
    "\n",
    "// === LOAD AND BRODCAST STOPWORDS ===\n",
    "val stopwords = sc.textFile(stopwordPath).collect().toSet\n",
    "val stopwordsBroadcast = sc.broadcast(stopwords)\n",
    "\n",
    "// faster parsing the file by defineing the structure of the json\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "//define sructure\n",
    "val reviewSchema = StructType(Seq(\n",
    "  StructField(\"reviewerID\"    , StringType),     // e.g. \"A2SUAM1J3GNN3B\"\n",
    "  StructField(\"asin\"          , StringType),     // product ID\n",
    "  StructField(\"reviewerName\"  , StringType),\n",
    "  StructField(\"helpful\"       , ArrayType(IntegerType)), // [a,b]\n",
    "  StructField(\"reviewText\"    , StringType),     // full body\n",
    "  StructField(\"overall\"       , DoubleType),     // rating 1-5 (float in source)\n",
    "  StructField(\"summary\"       , StringType),     // review title\n",
    "  StructField(\"unixReviewTime\", LongType),\n",
    "  StructField(\"reviewTime\"    , StringType),\n",
    "  StructField(\"category\"      , StringType)      // our label\n",
    "))\n",
    "\n",
    "//parse file\n",
    "val reviews = spark.read\n",
    "  .schema(reviewSchema)                 \n",
    "  .option(\"mode\",\"DROPMALFORMED\")       // skip corrupt lines\n",
    "  .json(inputPath)\n",
    "  .filter($\"category\".isNotNull)        \n",
    "  .select(\"category\",\"reviewText\",\"summary\") \n",
    "  .cache()\n",
    "val parsed = reviews.as[(String,String,String)].rdd\n",
    "// At this point the summary field will be omitted. If it is relevant, we need to merge it to the text.\n",
    "\n",
    "var end = System.nanoTime()\n",
    "var durationMs = (end - start) / 1e6\n",
    "println(f\"Runtime Parsing: $durationMs%.2f ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fea986-da82-4c06-85b0-05d7a1818d69",
   "metadata": {},
   "source": [
    "Token & Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3ecd7a-6e2d-45be-baa9-72c566fbc8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime tokenizer: 197.32 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "start: Long = 12150575032839522\n",
       "tokenize: (text: String, stopwords: Set[String])Set[String]\n",
       "tokenized: org.apache.spark.rdd.RDD[(String, Set[String])] = MapPartitionsRDD[13] at map at <console>:48\n",
       "end: Long = 12150575230160339\n",
       "durationMs: Double = 197.320817\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// === TOKENIZER ===\n",
    "start = System.nanoTime()\n",
    "\n",
    "//Define tokenize method\n",
    "//removes stopwords, replaces all delims with a whitespace and splits at ehitespaces after. Also transformes to lower case\n",
    "def tokenize(text: String, stopwords: Set[String]): Set[String] = {\n",
    "  if (text == null) return Set.empty\n",
    "  val cleaned = text.toLowerCase.replaceAll(DELIMS, \" \")\n",
    "  cleaned.split(\"\\\\s+\").filter(t => t.length > 1 && !stopwords.contains(t)).toSet\n",
    "}\n",
    "\n",
    "// call tokenize on parse json and stopwords\n",
    "val tokenized = parsed.map {\n",
    "  case (category, text, summary) =>\n",
    "    val tokens = tokenize(text, stopwordsBroadcast.value)\n",
    "    (category, tokens)\n",
    "}\n",
    "\n",
    "end = System.nanoTime()\n",
    "durationMs = (end - start) / 1e6\n",
    "println(f\"Runtime tokenizer: $durationMs%.2f ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df60927-41a6-47bd-98e1-7b96a5fde564",
   "metadata": {},
   "source": [
    "# Counting and cacluating chi²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbb5ee-7c45-48ff-b1c5-0180c9681c89",
   "metadata": {},
   "source": [
    "At this time this is the bottleneck, i.e the counting of the totalDocs etc. need to be adjusted fot the large dataset (>1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69428a9-0175-456c-8f60-ffa29397d42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Runtime: 13123.87 ms\n",
      "Chi-sq\n",
      "CHi Runtime: 14.19 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "start: Long = 12150589488375187\n",
       "tokenCatAndDocStats: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[15] at reduceByKey at <console>:42\n",
       "docCounts: scala.collection.Map[String,Int] = Map(Kindle_Store -> 3205, Electronic -> 7825, Automotive -> 1374, Pet_Supplie -> 1235, Clothing_Shoes_and_Jewelry -> 5749, Baby -> 916, Grocery_and_Gourmet_Food -> 1297, Musical_Instrument -> 500, Movies_and_TV -> 4607, Book -> 22507, Tools_and_Home_Improvement -> 1926, Sports_and_Outdoor -> 3269, CDs_and_Vinyl -> 3749, Home_and_Kitche -> 4254, Apps_for_Android -> 2638, Office_Product -> 1243, Digital_Music -> 836, Health_and_Personal_Care -> 2982, Cell_Phones_and_Accessorie -> 3447, Beauty -> 2023, Toys_and_Game -> 2253, Patio_Lawn_and_Garde -> 994)\n",
       "totalDocs: Int = 78829\n",
       "docCountsBroadcast:...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = System.nanoTime()\n",
    "\n",
    "val tokenCatAndDocStats = tokenized.flatMap {\n",
    "  case (cat, tokens) =>\n",
    "    val tokenSet = tokens.toSet\n",
    "    val tokenPairs = tokenSet.map(token => ((token, cat), 1))\n",
    "    val docMarker = Seq(((\"!DOC_COUNT\", cat), 1))\n",
    "    tokenPairs.toSeq ++ docMarker\n",
    "}.reduceByKey(_ + _)\n",
    "\n",
    "val docCounts = tokenCatAndDocStats\n",
    "  .filter(_._1._1 == \"!DOC_COUNT\")\n",
    "  .map { case ((_, cat), count) => (cat, count) }\n",
    "  .collectAsMap()\n",
    "\n",
    "val totalDocs = docCounts.values.sum\n",
    "val docCountsBroadcast = sc.broadcast(docCounts)\n",
    "val totalDocsBroadcast = sc.broadcast(totalDocs)\n",
    "\n",
    "val tokenCatCounts = tokenCatAndDocStats\n",
    "  .filter(_._1._1 != \"!DOC_COUNT\")\n",
    "\n",
    "val tokenTotals = tokenCatCounts\n",
    "  .map { case ((token, _), count) => (token, count) }\n",
    "  .reduceByKey(_ + _)\n",
    "  .collectAsMap()\n",
    "val tokenTotalsBroadcast = sc.broadcast(tokenTotals)\n",
    "\n",
    "\n",
    "end = System.nanoTime()\n",
    "durationMs = (end - start) / 1e6\n",
    "println(f\"Count Runtime: $durationMs%.2f ms\")\n",
    "start = System.nanoTime()\n",
    "\n",
    "start = System.nanoTime()\n",
    "// === CHI-SQUARE CALCULATION ===\n",
    "val N = totalDocsBroadcast.value.toDouble\n",
    "println(\"Chi-sq\")\n",
    "val chi2Scores = tokenCatCounts.map {\n",
    "  case ((token, cat), aCount) =>\n",
    "    val A = aCount.toDouble\n",
    "    val T = tokenTotalsBroadcast.value.getOrElse(token, 0).toDouble\n",
    "    val C = docCountsBroadcast.value.getOrElse(cat, 0).toDouble\n",
    "\n",
    "\n",
    "    val B = T - A\n",
    "    val D = N - C - B - A\n",
    "    val denom = (A + B) * (C + D) * (A + C) * (B + D)\n",
    "    val chi2 = if (denom == 0) 0.0 else N * math.pow((A * D - B * C), 2) / denom\n",
    "    (cat, (token, chi2))\n",
    "}\n",
    "\n",
    "end = System.nanoTime()\n",
    "durationMs = (end - start) / 1e6\n",
    "println(f\"CHi Runtime: $durationMs%.2f ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc18a6fe-ad2a-4c7a-a132-63d44e043faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topTokensPerCategory: org.apache.spark.rdd.RDD[(String, Seq[(String, Double)])] = MapPartitionsRDD[23] at mapValues at <console>:34\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//get top k elements\n",
    "val topTokensPerCategory = chi2Scores\n",
    "  .groupByKey()\n",
    "  .mapValues(iter => iter.toSeq.sortBy(-_._2).take(TOP_K))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056b527-d211-4e7c-b005-e39cbdb1b44f",
   "metadata": {},
   "source": [
    "Export Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5975097b-2de1-41d8-bc62-96975f7eda8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/09 23:48:49 WARN DAGScheduler: Broadcasting large task binary with size 1544.4 KiB\n",
      "25/05/09 23:48:51 WARN DAGScheduler: Broadcasting large task binary with size 1544.8 KiB\n",
      "Total Runtime: 29367.07 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import scala.reflect.io.File\n",
       "output: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[24] at map at <console>:36\n",
       "file: scala.reflect.io.File = output_rdd.txt\n",
       "mergedVocab: Array[String] = Array(acdelco, acne, acoustic, acre, acted, acting, action, actor, actors, actress, acura, adapter, addario, addicted, addicting, addictive, adjustment, adorable, ads, adventure, aftertaste, aired, airsoft, akai, albums, almonds, alpha, alternator, altima, ammo, amp, amplitube, android, animated, animation, anime, answering, antenna, ants, appetite, apple, apps, aquarium, ar, arch, arnley, aroma, arrangements, articulation, artisan, artist, artists, asus, atv, audio, author, authors, avent, avery, awesome, babies, back, backpacking, bag, bait, baking, ball, ballad, ballads, ballasts, balls, band, ban...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.reflect.io.File\n",
    "\n",
    "//format output as: <category> [term:chi2]\n",
    "val output = topTokensPerCategory.map { case (category, terms) =>\n",
    "  val formattedTerms = terms.map { case (term, chi2) =>\n",
    "    s\"$term:$chi2\"\n",
    "  }.mkString(\" \")\n",
    "  s\"<$category> $formattedTerms\"\n",
    "}\n",
    "\n",
    "//create and save to output file\n",
    "val file = File(\"output_rdd.txt\")\n",
    "file.writeAll(output.collect().mkString(\"\\n\"))\n",
    "\n",
    "// merging vocabluary\n",
    "val mergedVocab = topTokensPerCategory.flatMap(_._2.map(_._1)).distinct().collect().sorted\n",
    "\n",
    "// Append the sorted vocab to file\n",
    "file.appendAll(\"\\n\" + mergedVocab.mkString(\" \"))\n",
    "\n",
    "end = System.nanoTime()\n",
    "durationMs = (end - total) / 1e6\n",
    "println(f\"Total Runtime: $durationMs%.2f ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
