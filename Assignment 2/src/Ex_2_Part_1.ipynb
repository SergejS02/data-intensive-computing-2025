{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497c3fe5-37f4-4ea6-a20e-4436fdaa4e9f",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739efa21-7626-4780-811d-c937ed8a62cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://lbdmg01.datalab.novalocal:9999/proxy/application_1745308556449_5876\n",
       "SparkContext available as 'sc' (version = 3.3.4, master = yarn, app id = application_1745308556449_5876)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import scala.util.parsing.json.JSON\n",
       "import scala.util.matching.Regex\n",
       "import org.apache.spark.sql.types._\n",
       "import scala.reflect.io.File\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// --- IMPORTS ---\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import scala.util.parsing.json.JSON\n",
    "import scala.util.matching.Regex\n",
    "import org.apache.spark.sql.types._\n",
    "import scala.reflect.io.File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4918299f-ea4e-464e-9b64-6077a47b7f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwordPath: String = Exercise_1/stopwords.txt\n",
       "DELIMS: String = [()\\[\\]{}.!?,;:+=\\-_\"'`~#@&*%€§\\\\/0-9]+\n",
       "TOP_K: Int = 75\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopwordPath = \"Exercise_1/stopwords.txt\"\n",
    "val DELIMS = \"[()\\\\[\\\\]{}.!?,;:+=\\\\-_\\\"'`~#@&*%€§\\\\\\\\/0-9]+\"\n",
    "val TOP_K = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ee1d6-03c3-43e6-abc3-1ba33da61a18",
   "metadata": {},
   "source": [
    "# Ver1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74279e20-d3fd-44cd-813f-0c72d640acf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputPath: String = hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\n",
       "outputPath: String = output_rdd_full.txt\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val inputPath = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "val inputPath = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "val outputPath = \"output_rdd_full.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664a093-3058-4871-b315-7f01e71fbf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Time: 11.764 sec\n",
      "Tokenizer Time: 0.764 sec\n",
      "Counting\n",
      "   tokenCatAndDocStats: 0.097 sec\n",
      "   docCounts: 3886.985 sec\n",
      "   tokenCatCounts: 0.011 sec\n"
     ]
    }
   ],
   "source": [
    "var start0 = System.nanoTime()\n",
    "\n",
    "// --- LOAD STOPWORDS ---\n",
    "var start = System.nanoTime()\n",
    "val stopwords = sc.textFile(stopwordPath).collect().toSet\n",
    "val stopwordsBroadcast = sc.broadcast(stopwords)\n",
    "\n",
    "// --- PARSE FILE ---\n",
    "// faster parsing the file by defining the structure of the json\n",
    "val reviewSchema = StructType(Seq(\n",
    "  StructField(\"reviewerID\"    , StringType),     // e.g. \"A2SUAM1J3GNN3B\"\n",
    "  StructField(\"asin\"          , StringType),     // product ID\n",
    "  StructField(\"reviewerName\"  , StringType),\n",
    "  StructField(\"helpful\"       , ArrayType(IntegerType)), // [a,b]\n",
    "  StructField(\"reviewText\"    , StringType),     // full body\n",
    "  StructField(\"overall\"       , DoubleType),     // rating 1-5 (float in source)\n",
    "  StructField(\"summary\"       , StringType),     // review title\n",
    "  StructField(\"unixReviewTime\", LongType),\n",
    "  StructField(\"reviewTime\"    , StringType),\n",
    "  StructField(\"category\"      , StringType)      // our label\n",
    "))\n",
    "\n",
    "val reviews = spark.read\n",
    "  .schema(reviewSchema)                 \n",
    "  .option(\"mode\",\"DROPMALFORMED\")       // skip corrupt lines\n",
    "  .json(inputPath)\n",
    "  .filter($\"category\".isNotNull)        \n",
    "  .select(\"category\",\"reviewText\",\"summary\") \n",
    "  .cache()\n",
    "val parsed = reviews.as[(String,String,String)].rdd\n",
    "// At this point the summary field will be omitted. If it is relevant, we need to merge it to the text.\n",
    "\n",
    "println(f\"Parsing Time: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "// --- TOKENIZER ---\n",
    "start = System.nanoTime()\n",
    "\n",
    "//Define tokenize method\n",
    "//removes stopwords, replaces all delims with a whitespace and splits at ehitespaces after. Also transformes to lower case\n",
    "def tokenize(text: String, stopwords: Set[String]): Set[String] = {\n",
    "  if (text == null) return Set.empty\n",
    "  val cleaned = text.toLowerCase.replaceAll(DELIMS, \" \")\n",
    "  cleaned.split(\"\\\\s+\").filter(t => t.length > 1 && !stopwords.contains(t)).toSet\n",
    "}\n",
    "\n",
    "// call tokenize on parse json and stopwords\n",
    "val tokenized = parsed.map {\n",
    "  case (category, text, summary) =>\n",
    "    val tokens = tokenize(text, stopwordsBroadcast.value)\n",
    "    (category, tokens)\n",
    "}\n",
    "\n",
    "println(f\"Tokenizer Time: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "// --- COUNTING ---\n",
    "println(\"Counting\")\n",
    "start = System.nanoTime()\n",
    "val tokenCatAndDocStats = tokenized.flatMap {\n",
    "  case (cat, tokens) =>\n",
    "    val tokenSet = tokens.toSet\n",
    "    val tokenPairs = tokenSet.map(token => ((token, cat), 1))\n",
    "    val docMarker = Seq(((\"!DOC_COUNT\", cat), 1))\n",
    "    tokenPairs.toSeq ++ docMarker\n",
    "}.reduceByKey(_ + _)\n",
    "println(f\"   tokenCatAndDocStats: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "start = System.nanoTime()\n",
    "val docCounts = tokenCatAndDocStats\n",
    "  .filter(_._1._1 == \"!DOC_COUNT\")\n",
    "  .map { case ((_, cat), count) => (cat, count) }\n",
    "  .collectAsMap()\n",
    "val totalDocs = docCounts.values.sum\n",
    "val docCountsBroadcast = sc.broadcast(docCounts)\n",
    "val totalDocsBroadcast = sc.broadcast(totalDocs)\n",
    "println(f\"   docCounts: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "start = System.nanoTime()\n",
    "val tokenCatCounts = tokenCatAndDocStats\n",
    "  .filter(_._1._1 != \"!DOC_COUNT\")\n",
    "println(f\"   tokenCatCounts: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "start = System.nanoTime()\n",
    "val tokenTotals = tokenCatCounts\n",
    "  .map { case ((token, _), count) => (token, count) }\n",
    "  .reduceByKey(_ + _)\n",
    "  .collectAsMap()\n",
    "val tokenTotalsBroadcast = sc.broadcast(tokenTotals)\n",
    "println(f\"   tokenTotals: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "// --- CHI-SQUARE CALCULATION ---\n",
    "start = System.nanoTime()\n",
    "val N = totalDocsBroadcast.value.toDouble\n",
    "val chi2Scores = tokenCatCounts.map {\n",
    "  case ((token, cat), aCount) =>\n",
    "    val A = aCount.toDouble\n",
    "    val T = tokenTotalsBroadcast.value.getOrElse(token, 0).toDouble\n",
    "    val C = docCountsBroadcast.value.getOrElse(cat, 0).toDouble\n",
    "    val B = T - A\n",
    "    val D = N - C - B - A\n",
    "    val denom = (A + B) * (C + D) * (A + C) * (B + D)\n",
    "    val chi2 = if (denom == 0) 0.0 else N * math.pow((A * D - B * C), 2) / denom\n",
    "    (cat, (token, chi2))\n",
    "}\n",
    "println(f\"Chi-Square Time: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "// --- PREPARE OUTPUT\n",
    "start = System.nanoTime()\n",
    "// get top k elements\n",
    "val topTokensPerCategory = chi2Scores\n",
    "  .groupByKey()\n",
    "  .mapValues(iter => iter.toSeq.sortBy(-_._2).take(TOP_K))\n",
    "\n",
    "// merging vocabluary\n",
    "val mergedVocab = topTokensPerCategory.flatMap(_._2.map(_._1)).distinct().collect().sorted\n",
    "\n",
    "println(f\"Prepare Output: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "// --- EXPORT ---\n",
    "start = System.nanoTime()\n",
    "// format output as: <category> [term:chi2]\n",
    "val output = topTokensPerCategory.map { case (category, terms) =>\n",
    "  val formattedTerms = terms.map { case (term, chi2) =>\n",
    "    s\"$term:$chi2\"\n",
    "  }.mkString(\" \")\n",
    "  s\"<$category> $formattedTerms\"\n",
    "}\n",
    "\n",
    "//create and save to output file\n",
    "val file = File(outputPath)\n",
    "file.writeAll(output.collect().mkString(\"\\n\"))\n",
    "\n",
    "// append the sorted vocab to file\n",
    "file.appendAll(\"\\n\" + mergedVocab.mkString(\" \"))\n",
    "\n",
    "println(f\"Export Output: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "println(f\"Total Runtime: ${(System.nanoTime() - start0) / 1e9}%.3f sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40942a78-2b5e-4a2c-99fa-c793dfd62f54",
   "metadata": {},
   "source": [
    "Devset\n",
    "\n",
    "Parsing Time: 0.965 sec\n",
    "Tokenizer Time: 0.039 sec\n",
    "Counting\n",
    "   tokenCatAndDocStats: 0.067 sec\n",
    "   docCounts: 10.896 sec\n",
    "   tokenCatCounts: 0.010 sec\n",
    "   tokenTotals: 2.542 sec\n",
    "Chi-Square Time: 0.020 sec\n",
    "Prepare Output: 1.966 sec\n",
    "Export Output: 0.671 sec\n",
    "Total Runtime: 17.180 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5204d-1484-46d0-a46c-c779f7b076c3",
   "metadata": {},
   "source": [
    "# Ver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029a1616-4b64-467a-8952-cd4cffc9ffd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputPath: String = hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\n",
       "outputPath: String = output_rdd2_full.txt\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val inputPath = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "val inputPath = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "val outputPath = \"output_rdd2_full.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f00f3a0-f94e-4021-88ce-7e18526426b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Time: 0.667 sec\n",
      "Tokenizer Time: 0.041 sec\n",
      "Counting\n",
      "   tokenCatAndDocStats: 0.027 sec\n",
      "   docCounts: 7.513 sec\n",
      "   tokenCatCounts: 0.013 sec\n",
      "   tokenTotalsBroadcast: 1.638 sec\n",
      "Chi-Square Time: 0.012 sec\n",
      "Top-K Selection Time: 0.079 sec\n",
      "Merge Vocab Time: 10.756 sec\n",
      "Export Time: 0.597 sec\n",
      "Total Runtime: 21.361 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "start0: Long = 12238132894274467\n",
       "start: Long = 12238153658092396\n",
       "stopwords: scala.collection.immutable.Set[String] = Set(serious, latterly, absorbs, looks, particularly, used, e, printer, down, regarding, entirely, regardless, moreover, please, read, ourselves, able, behind, for, despite, s, maybe, viz, further, corresponding, x, any, wherein, across, name, allows, this, instead, in, taste, ought, myself, have, your, off, once, are, is, mon, his, oh, why, rd, knows, bulbs, too, among, course, greetings, somewhat, bibs, everyone, seen, likely, said, try, already, soon, nobody, got, given, song, using, less, am, consider, hence, than, n, accordingly, four, anyhow, want, three, forth, whereby, himself, specify, yes, throughout, inasmuch, but, whether, sure, below, aren, co, best, plus, bec...\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var start0 = System.nanoTime()\n",
    "\n",
    "// --- LOAD STOPWORDS ---\n",
    "var start = System.nanoTime()\n",
    "val stopwords = sc.textFile(stopwordPath).collect().toSet\n",
    "val stopwordsBroadcast = sc.broadcast(stopwords)\n",
    "\n",
    "// --- PARSE FILE --- \n",
    "//faster parsing the file by defining the structure of the json\n",
    "val reviewSchema = StructType(Seq(\n",
    "  StructField(\"reviewerID\"    , StringType),     // e.g. \"A2SUAM1J3GNN3B\"\n",
    "  StructField(\"asin\"          , StringType),     // product ID\n",
    "  StructField(\"reviewerName\"  , StringType),\n",
    "  StructField(\"helpful\"       , ArrayType(IntegerType)), // [a,b]\n",
    "  StructField(\"reviewText\"    , StringType),     // full body\n",
    "  StructField(\"overall\"       , DoubleType),     // rating 1-5 (float in source)\n",
    "  StructField(\"summary\"       , StringType),     // review title\n",
    "  StructField(\"unixReviewTime\", LongType),\n",
    "  StructField(\"reviewTime\"    , StringType),\n",
    "  StructField(\"category\"      , StringType)      // our label\n",
    "))\n",
    "val reviews = spark.read\n",
    "  .schema(reviewSchema)                 \n",
    "  .option(\"mode\",\"DROPMALFORMED\")       // skip corrupt lines\n",
    "  .json(inputPath)\n",
    "  .filter($\"category\".isNotNull)        \n",
    "  // .select($\"category\", concat_ws(\" \", $\"reviewText\", $\"summary\").as(\"text\")) // takes too long even on devset\n",
    "  .select($\"category\", $\"reviewText\")  // only reviewText\n",
    "  .cache()\n",
    "val parsed = reviews.as[(String,String)].rdd\n",
    "println(f\"Parsing Time: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "// --- TOKENIZER ---\n",
    "start = System.nanoTime()\n",
    "//removes stopwords, replaces all delims with a whitespace and splits at ehitespaces after. Also transformes to lower case\n",
    "def tokenize(text: String, stopwords: Set[String]): Set[String] = {\n",
    "  if (text == null) Set.empty\n",
    "  else text\n",
    "    .toLowerCase\n",
    "    .replaceAll(DELIMS, \" \") // use your defined DELIMS\n",
    "    .split(\"\\\\s+\")\n",
    "    .filter(t => t.length > 1 && !stopwords.contains(t))\n",
    "    .toSet\n",
    "}\n",
    "val tokenized = parsed.map {\n",
    "  case (category, text) =>\n",
    "    val tokens = tokenize(text, stopwordsBroadcast.value)\n",
    "    (category, tokens)\n",
    "}.cache()\n",
    "println(f\"Tokenizer Time: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "// --- COUNTING ---\n",
    "start = System.nanoTime()\n",
    "println(\"Counting\")\n",
    "val tokenCatAndDocStats = tokenized.flatMap {\n",
    "  case (cat, tokens) =>\n",
    "    val tokenSet = tokens.toSet\n",
    "    val tokenPairs = tokenSet.map(token => ((token, cat), 1))\n",
    "    val docMarker = Seq(((\"!DOC_COUNT\", cat), 1))\n",
    "    tokenPairs.toSeq ++ docMarker\n",
    "}.reduceByKey(_ + _).cache()\n",
    "println(f\"   tokenCatAndDocStats: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "start = System.nanoTime()\n",
    "val docCounts = tokenCatAndDocStats\n",
    "  .filter { case ((key, _), _) => key == \"!DOC_COUNT\" }\n",
    "  .map { case ((_, cat), count) => (cat, count) }\n",
    "  .reduceByKey(_ + _)  // aggregate locally on each node to reduce data transferred\n",
    "  .collectAsMap()\n",
    "println(f\"   docCounts: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "val totalDocs = docCounts.values.sum\n",
    "val docCountsBroadcast = sc.broadcast(docCounts)\n",
    "val totalDocsBroadcast = sc.broadcast(totalDocs)\n",
    "\n",
    "start = System.nanoTime()\n",
    "val tokenCatCounts = tokenCatAndDocStats\n",
    "    .filter(_._1._1 != \"!DOC_COUNT\")\n",
    "    .cache()\n",
    "println(f\"   tokenCatCounts: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "start = System.nanoTime()\n",
    "val tokenTotals = tokenCatCounts\n",
    "  .map { case ((token, _), count) => (token, count) }\n",
    "  .reduceByKey(_ + _)\n",
    "  .collect()  // Not collectAsMap()\n",
    "val tokenTotalsMap = tokenTotals.toMap  // Convert locally\n",
    "val tokenTotalsBroadcast = sc.broadcast(tokenTotalsMap)\n",
    "println(f\"   tokenTotalsBroadcast: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "// --- CHI-SQUARE CALCULATION ---\n",
    "start = System.nanoTime()\n",
    "val N = totalDocsBroadcast.value.toDouble\n",
    "val chi2Scores = tokenCatCounts.map {\n",
    "  case ((token, cat), aCount) =>\n",
    "    val A = aCount.toDouble\n",
    "    val T = tokenTotalsBroadcast.value.getOrElse(token, 0).toDouble\n",
    "    val C = docCountsBroadcast.value.getOrElse(cat, 0).toDouble\n",
    "    val B = T - A\n",
    "    val D = N - C - B - A\n",
    "    val denom = (A + B) * (C + D) * (A + C) * (B + D)\n",
    "    val chi2 = if (denom == 0) 0.0 else N * math.pow((A * D - B * C), 2) / denom\n",
    "    (cat, (token, chi2))\n",
    "}.cache()\n",
    "println(f\"Chi-Square Time: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "// --- TOP K TERMS PER CATEGORY ---\n",
    "start = System.nanoTime()\n",
    "val partitioned = chi2Scores.repartition(200)\n",
    "val topTokensPerCategory = partitioned\n",
    "  .groupByKey()\n",
    "  .mapValues(iter => iter.toSeq.sortBy(-_._2).take(TOP_K))\n",
    "  .cache()\n",
    "println(f\"Top-K Selection Time: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "start = System.nanoTime()\n",
    "// merging vocabluary\n",
    "val mergedVocab = topTokensPerCategory.flatMap(_._2.map(_._1)).distinct().collect().sorted\n",
    "println(f\"Merge Vocab Time: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "start = System.nanoTime()\n",
    "//format output as: <category> [term:chi2]\n",
    "val output = topTokensPerCategory.map { case (category, terms) =>\n",
    "  val formattedTerms = terms.map { case (term, chi2) =>\n",
    "    s\"$term:$chi2\"\n",
    "  }.mkString(\" \")\n",
    "  s\"<$category> $formattedTerms\"\n",
    "}\n",
    "\n",
    "//create and save to output file\n",
    "val file = File(outputPath)\n",
    "file.writeAll(output.collect().mkString(\"\\n\"))\n",
    "\n",
    "// Append the sorted vocab to file\n",
    "file.appendAll(\"\\n\" + mergedVocab.mkString(\" \"))\n",
    "\n",
    "println(f\"Export Time: ${(System.nanoTime() - start) / 1e9}%.3f sec\")\n",
    "\n",
    "println(f\"Total Runtime: ${(System.nanoTime() - start0) / 1e9}%.3f sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340e6da-df84-4b62-b7f6-05b8a95ebb49",
   "metadata": {},
   "source": [
    "dev set\n",
    "\n",
    "Parsing Time: 0.667 sec\n",
    "Tokenizer Time: 0.041 sec\n",
    "Counting\n",
    "   tokenCatAndDocStats: 0.027 sec\n",
    "   docCounts: 7.513 sec\n",
    "   tokenCatCounts: 0.013 sec\n",
    "   tokenTotalsBroadcast: 1.638 sec\n",
    "Chi-Square Time: 0.012 sec\n",
    "Top-K Selection Time: 0.079 sec\n",
    "Merge Vocab Time: 10.756 sec\n",
    "Export Time: 0.597 sec\n",
    "Total Runtime: 21.361 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f34876-093c-4ffe-b8ba-aff45cc113d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
