{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59975bcd-3c0d-4fb9-b48a-96278518430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:13:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/10 02:13:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/10 02:13:57 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/10 02:13:57 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/10 02:13:57 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/10 02:13:57 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/10 02:13:57 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/10 02:13:57 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/10 02:13:59 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#open spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"A2-Part2-Pipeline\").getOrCreate()\n",
    "\n",
    "#load reviews and stopwords\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    RAW_PATH = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "else:\n",
    "    RAW_PATH = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "stopwordsPath = \"Exercise_1/assets/stopwords.txt\"\n",
    "#define structure of json for faster reading\n",
    "from pyspark.sql import types as T\n",
    "review_schema = T.StructType([\n",
    "     T.StructField(\"reviewerID\",      T.StringType(),  True),\n",
    "     T.StructField(\"asin\",            T.StringType(),  True),\n",
    "     T.StructField(\"reviewerName\",    T.StringType(),  True),\n",
    "     T.StructField(\"helpful\",         T.ArrayType(T.IntegerType()), True),\n",
    "     T.StructField(\"reviewText\",      T.StringType(),  True),\n",
    "     T.StructField(\"overall\",         T.FloatType(),   True),\n",
    "     T.StructField(\"summary\",         T.StringType(),  True),\n",
    "     T.StructField(\"unixReviewTime\",  T.LongType(),    True),\n",
    "     T.StructField(\"reviewTime\",      T.StringType(),  True),\n",
    "     T.StructField(\"category\",        T.StringType(),  True),\n",
    " ])\n",
    "#read and select category and review\n",
    "df = (\n",
    "    spark.read\n",
    "         .schema(review_schema)\n",
    "         .json(RAW_PATH)\n",
    "         .selectExpr(\"reviewText AS text\",\n",
    "                     \"category AS label\")\n",
    "    .na.drop(subset=[\"text\", \"label\"])\n",
    ")\n",
    "\n",
    "# reading the stopwords\n",
    "stopwords = spark.sparkContext.textFile(stopwordsPath).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd0c967-61d5-4092-8f58-c6df611a38d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Val: 63,040  Â·  Test: 15,789\n",
      "Grid size: 12 parameter combos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:25:09 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:25:14 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:25:20 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:25:20 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:25:25 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n",
      "25/05/10 02:25:25 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n",
      "25/05/10 02:25:25 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:25:34 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n",
      "25/05/10 02:25:34 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:25:39 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:25:48 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:25:58 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>   (0 + 1) / 2][Stage 63:>   (0 + 0) / 2][Stage 64:==> (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:09 WARN TaskSetManager: Lost task 0.0 in stage 61.0 (TID 86) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:26:10 WARN TaskSetManager: Lost task 1.0 in stage 61.0 (TID 88) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>   (0 + 1) / 2][Stage 63:>   (0 + 1) / 2][Stage 65:>   (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:21 WARN TaskSetManager: Lost task 0.0 in stage 63.0 (TID 93) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>   (0 + 2) / 2][Stage 63:>   (0 + 0) / 2][Stage 65:>   (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:21 WARN TaskSetManager: Lost task 0.3 in stage 61.0 (TID 94) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:26:21 ERROR TaskSetManager: Task 0 in stage 61.0 failed 4 times; aborting job\n",
      "25/05/10 02:26:21 WARN TaskSetManager: Lost task 1.3 in stage 61.0 (TID 95) (lbdwo15.datalab.novalocal executor 2): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>   (0 + 1) / 2][Stage 65:>   (0 + 1) / 2][Stage 66:>   (0 + 0) / 2]"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1024.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 61.0 failed 4 times, most recent failure: Lost task 0.3 in stage 61.0 (TID 94) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2604)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2603)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2603)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1178)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1111)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1545)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1533)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3908)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3160)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3898)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3896)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3896)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3160)\n\tat org.apache.spark.ml.feature.Selector.getTopIndices$1(Selector.scala:216)\n\tat org.apache.spark.ml.feature.Selector.fit(Selector.scala:222)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:111)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:49)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 121\u001b[0m\n\u001b[1;32m    110\u001b[0m tvs \u001b[38;5;241m=\u001b[39m TrainValidationSplit(\n\u001b[1;32m    111\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipe,\n\u001b[1;32m    112\u001b[0m     estimatorParamMaps\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     seed\u001b[38;5;241m=\u001b[39mSEED,\n\u001b[1;32m    116\u001b[0m     parallelism\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)            \u001b[38;5;66;03m# adjust to cluster resources\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#  7.  Fit grid & choose best model\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m tvs_model \u001b[38;5;241m=\u001b[39m \u001b[43mtvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_val_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m best_model \u001b[38;5;241m=\u001b[39m tvs_model\u001b[38;5;241m.\u001b[39mbestModel\n\u001b[1;32m    123\u001b[0m best_params \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mname: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m best_model\u001b[38;5;241m.\u001b[39mstages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# OvR â LinearSVCModel\u001b[39;00m\n\u001b[1;32m    124\u001b[0m                                        \u001b[38;5;241m.\u001b[39mgetClassifier()\n\u001b[1;32m    125\u001b[0m                                        \u001b[38;5;241m.\u001b[39mextractParamMap()\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:1464\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1462\u001b[0m pool \u001b[38;5;241m=\u001b[39m ThreadPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetParallelism(), numModels))\n\u001b[1;32m   1463\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m numModels\n\u001b[0;32m-> 1464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m   1465\u001b[0m     metrics[j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:870\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:1464\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   1462\u001b[0m pool \u001b[38;5;241m=\u001b[39m ThreadPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetParallelism(), numModels))\n\u001b[1;32m   1463\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m numModels\n\u001b[0;32m-> 1464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m   1465\u001b[0m     metrics[j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/util.py:337\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    336\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sw/venv/python39/python/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/sw/venv/python39/python/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1024.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 61.0 failed 4 times, most recent failure: Lost task 0.3 in stage 61.0 (TID 94) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2604)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2603)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2603)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1178)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1111)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1545)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1533)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3908)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3160)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3898)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3896)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3896)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3160)\n\tat org.apache.spark.ml.feature.Selector.getTopIndices$1(Selector.scala:216)\n\tat org.apache.spark.ml.feature.Selector.fit(Selector.scala:222)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:111)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:49)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:24 WARN TaskSetManager: Lost task 0.0 in stage 65.0 (TID 97) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>   (0 + 2) / 2][Stage 65:>   (0 + 0) / 2][Stage 66:>   (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:25 WARN TaskSetManager: Lost task 1.0 in stage 63.0 (TID 98) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>   (0 + 1) / 2][Stage 65:>   (0 + 1) / 2][Stage 66:>   (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:31 ERROR TaskSetManager: Task 0 in stage 63.0 failed 4 times; aborting job\n",
      "25/05/10 02:26:31 WARN TaskSetManager: Lost task 1.2 in stage 63.0 (TID 104) (lbdwo17.datalab.novalocal executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:>   (0 + 2) / 2][Stage 66:>   (0 + 0) / 2][Stage 68:>   (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:32 WARN TaskSetManager: Lost task 1.0 in stage 65.0 (TID 105) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:>   (0 + 1) / 2][Stage 66:>   (0 + 1) / 2][Stage 68:>   (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:35 WARN TaskSetManager: Lost task 0.3 in stage 65.0 (TID 107) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:26:35 ERROR TaskSetManager: Task 0 in stage 65.0 failed 4 times; aborting job\n",
      "25/05/10 02:26:35 WARN TaskSetManager: Lost task 1.1 in stage 65.0 (TID 108) (lbdwo17.datalab.novalocal executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:>   (0 + 2) / 2][Stage 70:==> (1 + 0) / 2][Stage 72:>   (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:51 WARN TaskSetManager: Lost task 0.0 in stage 67.0 (TID 113) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:>   (0 + 2) / 2][Stage 69:==> (1 + 0) / 2][Stage 70:==> (1 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:26:52 WARN TaskSetManager: Lost task 1.0 in stage 67.0 (TID 114) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:>   (0 + 2) / 2][Stage 71:==> (1 + 0) / 2][Stage 72:>   (0 + 0) / 2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:00 ERROR TaskSetManager: Task 1 in stage 67.0 failed 4 times; aborting job\n",
      "25/05/10 02:27:00 WARN TaskSetManager: Lost task 0.3 in stage 67.0 (TID 124) (lbdwo15.datalab.novalocal executor 2): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:18 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:24 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:27 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:29 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:32 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:34 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:37 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n",
      "25/05/10 02:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:50 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:27:56 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:06 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 99:>   (0 + 1) / 2][Stage 102:=> (1 + 1) / 2][Stage 104:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:08 WARN TaskSetManager: Lost task 0.0 in stage 99.0 (TID 171) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:28:10 WARN TaskSetManager: Lost task 1.0 in stage 99.0 (TID 175) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 99:>   (0 + 1) / 2][Stage 103:>  (0 + 0) / 2][Stage 104:>  (0 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:22 WARN TaskSetManager: Lost task 0.3 in stage 99.0 (TID 181) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:28:22 ERROR TaskSetManager: Task 0 in stage 99.0 failed 4 times; aborting job\n",
      "25/05/10 02:28:22 WARN TaskSetManager: Lost task 1.3 in stage 99.0 (TID 182) (lbdwo17.datalab.novalocal executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:>  (0 + 2) / 2][Stage 104:=> (1 + 0) / 2][Stage 106:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:24 WARN TaskSetManager: Lost task 1.0 in stage 103.0 (TID 184) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:>  (0 + 1) / 2][Stage 104:=> (1 + 1) / 2][Stage 106:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:25 WARN TaskSetManager: Lost task 0.0 in stage 103.0 (TID 183) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:>                (0 + 1) / 2][Stage 108:>                (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:35 WARN TaskSetManager: Lost task 1.3 in stage 103.0 (TID 190) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:28:35 ERROR TaskSetManager: Task 1 in stage 103.0 failed 4 times; aborting job\n",
      "25/05/10 02:28:35 WARN TaskSetManager: Lost task 0.3 in stage 103.0 (TID 191) (lbdwo15.datalab.novalocal executor 2): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:>  (0 + 1) / 2][Stage 106:>  (0 + 1) / 2][Stage 108:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:37 WARN TaskSetManager: Lost task 0.0 in stage 105.0 (TID 193) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:28:38 WARN TaskSetManager: Lost task 1.0 in stage 105.0 (TID 194) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:>  (0 + 1) / 2][Stage 106:=> (1 + 1) / 2][Stage 108:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:49 WARN TaskSetManager: Lost task 0.3 in stage 105.0 (TID 200) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:28:49 ERROR TaskSetManager: Task 0 in stage 105.0 failed 4 times; aborting job\n",
      "25/05/10 02:28:49 WARN TaskSetManager: Lost task 1.3 in stage 105.0 (TID 201) (lbdwo17.datalab.novalocal executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:>  (0 + 1) / 2][Stage 108:=> (1 + 1) / 2][Stage 110:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:58 WARN TaskSetManager: Lost task 0.0 in stage 107.0 (TID 204) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:>  (0 + 2) / 2][Stage 109:>  (0 + 0) / 2][Stage 110:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:28:59 WARN TaskSetManager: Lost task 1.0 in stage 107.0 (TID 205) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:>  (0 + 2) / 2][Stage 110:=> (1 + 0) / 2][Stage 112:>  (0 + 0) / 2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:29:08 WARN TaskSetManager: Lost task 0.3 in stage 107.0 (TID 213) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:29:08 ERROR TaskSetManager: Task 0 in stage 107.0 failed 4 times; aborting job\n",
      "25/05/10 02:29:08 WARN TaskSetManager: Lost task 1.3 in stage 107.0 (TID 214) (lbdwo17.datalab.novalocal executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:29:26 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:29:30 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:29:35 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:29:36 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n",
      "25/05/10 02:29:36 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:29:41 WARN DAGScheduler: Broadcasting large task binary with size 1034.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:29:48 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n",
      "25/05/10 02:29:48 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n",
      "25/05/10 02:29:48 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:29:56 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:>  (0 + 2) / 2][Stage 140:=> (1 + 0) / 2][Stage 142:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:30:07 WARN TaskSetManager: Lost task 0.0 in stage 137.0 (TID 259) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:>  (0 + 2) / 2][Stage 139:=> (1 + 0) / 2][Stage 140:=> (1 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:30:08 WARN TaskSetManager: Lost task 1.0 in stage 137.0 (TID 260) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:30:08 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:>  (0 + 2) / 2][Stage 141:>  (0 + 0) / 2][Stage 142:>  (0 + 0) / 2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:30:16 ERROR TaskSetManager: Task 1 in stage 137.0 failed 4 times; aborting job\n",
      "25/05/10 02:30:16 WARN TaskSetManager: Lost task 0.3 in stage 137.0 (TID 269) (lbdwo17.datalab.novalocal executor 1): TaskKilled (Stage cancelled)\n",
      "25/05/10 02:30:16 WARN DAGScheduler: Broadcasting large task binary with size 1038.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:>  (0 + 1) / 2][Stage 144:=> (1 + 1) / 2][Stage 146:>  (0 + 0) / 2]2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:30:35 WARN TaskSetManager: Lost task 0.0 in stage 143.0 (TID 276) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:>  (0 + 2) / 2][Stage 145:>  (0 + 0) / 2][Stage 146:>  (0 + 0) / 2]2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:30:36 WARN TaskSetManager: Lost task 1.0 in stage 143.0 (TID 277) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:>  (0 + 1) / 2][Stage 145:>  (0 + 1) / 2][Stage 146:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:30:39 WARN TaskSetManager: Lost task 0.0 in stage 145.0 (TID 279) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:>  (0 + 1) / 2][Stage 145:>  (0 + 1) / 2][Stage 146:>  (0 + 0) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:30:46 WARN TaskSetManager: Lost task 0.3 in stage 143.0 (TID 284) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:30:46 ERROR TaskSetManager: Task 0 in stage 143.0 failed 4 times; aborting job\n",
      "25/05/10 02:30:46 WARN TaskSetManager: Lost task 1.3 in stage 143.0 (TID 286) (lbdwo17.datalab.novalocal executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 145:>                (0 + 1) / 2][Stage 146:>                (0 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:30:48 WARN TaskSetManager: Lost task 1.0 in stage 145.0 (TID 287) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:30:51 WARN TaskSetManager: Lost task 0.3 in stage 145.0 (TID 289) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:30:51 ERROR TaskSetManager: Task 0 in stage 145.0 failed 4 times; aborting job\n",
      "25/05/10 02:30:51 WARN TaskSetManager: Lost task 1.1 in stage 145.0 (TID 290) (lbdwo17.datalab.novalocal executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 147:>                                                        (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/10 02:31:03 WARN TaskSetManager: Lost task 1.0 in stage 147.0 (TID 293) (lbdwo17.datalab.novalocal executor 1): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 3.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:31:04 WARN TaskSetManager: Lost task 0.0 in stage 147.0 (TID 292) (lbdwo15.datalab.novalocal executor 2): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 2.\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSq(ChiSqTest.scala:195)\n",
      "\tat org.apache.spark.mllib.stat.test.ChiSqTest$.$anonfun$chiSquaredSparseFeatures$15(ChiSqTest.scala:175)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.TopKSelector.offerAll(TopKSelector.java:248)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:785)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1539)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/05/10 02:31:08 ERROR TaskSetManager: Task 1 in stage 147.0 failed 4 times; aborting job\n",
      "25/05/10 02:31:08 WARN TaskSetManager: Lost task 0.2 in stage 147.0 (TID 298) (lbdwo15.datalab.novalocal executor 2): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#  Part 3 â Multiclass Text Classification with Linear-SVM  (PySpark)\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#\n",
    "#  Prerequisite: the DataFrame `df` from Part 2\n",
    "#                columns:  text(string) , label(string|int)\n",
    "#  Spark â¥ 3.2.0 recommended\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, CountVectorizer, IDF,\n",
    "    Normalizer, ChiSqSelector, StringIndexer\n",
    ")\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#  1.  Reproducible split: 80 % trainVal / 20 % test\n",
    "#      â¦ and inside the CV step below: another 80 % / 20 % split to get\n",
    "#      train vs. *validation* (â effective 64 % train Â· 16 % val Â· 20 % test)\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "SEED = 567\n",
    "train_val_df, test_df = df.randomSplit([0.8, 0.2], seed=SEED)\n",
    "print(f\"Train+Val: {train_val_df.count():,}  Â·  Test: {test_df.count():,}\")\n",
    "\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#  2.  String-labels â numeric (required for SVM)\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\").fit(df)\n",
    "\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#  3.  Shared preprocessing stages     (exactly as in Part 2)\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"text\", outputCol=\"tokens\",\n",
    "    pattern=r\"\"\"[ \\t0-9()\\[\\]{}.!?,;:+=\\-_\"'`~#@&*%â¬$Â§\\\\/]+\"\"\",\n",
    "    gaps=True, toLowercase=True)\n",
    "\n",
    "stopper = StopWordsRemover(inputCol=\"tokens\", outputCol=\"clean_tokens\")\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"clean_tokens\", outputCol=\"tf\",\n",
    "    minDF=2, vocabSize=50_000)        # keep generous ceiling\n",
    "\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "\n",
    "#  Normalise TF-IDF vectors\n",
    "normaliser = Normalizer(inputCol=\"tfidf\", outputCol=\"features\", p=2.0)\n",
    "\n",
    "#  Two ÏÂ² selectors (2 000 vs. 500 features)\n",
    "selector2k = ChiSqSelector(\n",
    "    numTopFeatures=2_000,\n",
    "    featuresCol=\"features\", outputCol=\"chi2_features\",\n",
    "    labelCol=\"label_idx\")\n",
    "\n",
    "selector500 = selector2k.copy({selector2k.numTopFeatures: 500})\n",
    "\n",
    "#  SVM wrapped for multi-class\n",
    "svm = LinearSVC(featuresCol=\"chi2_features\",\n",
    "                labelCol=\"label_idx\",\n",
    "                predictionCol=\"prediction\")\n",
    "\n",
    "ovr = OneVsRest(classifier=svm,\n",
    "                labelCol=\"label_idx\",\n",
    "                featuresCol=\"chi2_features\",\n",
    "                predictionCol=\"prediction\")\n",
    "\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#  4.  Pipeline (place-holders; grid will swap in selector2k vs. selector500)\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "pipe = Pipeline(stages=[\n",
    "    label_indexer,         # [0]\n",
    "    tokenizer, stopper, cv, idf, normaliser,\n",
    "    selector2k,            # [6] will be overridden by grid\n",
    "    ovr                    # [7]\n",
    "])\n",
    "\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#  5.  Param grid\n",
    "#      â¢ ÏÂ²: 2 000  vs  500\n",
    "#      â¢ regParam:  0.01, 0.1, 1.0\n",
    "#      â¢ standardization:  True / False\n",
    "#      â¢ maxIter:  50 / 200\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    # swap ÏÂ²-selector stage\n",
    "    #.addGrid(pipe.stages[6], selector2k)\n",
    "    # SVM hyper-params (access the underlying LinearSVC inside the OvR)\n",
    "    .addGrid(svm.regParam,        [0.01, 0.1, 1.0])\n",
    "    .addGrid(svm.standardization, [True, False])\n",
    "    .addGrid(svm.maxIter,         [50, 200])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(f\"Grid size: {len(param_grid):,} parameter combos\")\n",
    "\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#  6.  Train-validation split inside **TrainValidationSplit**\n",
    "#      (20 % of trainVal_df is held out for validation)\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_idx\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\")\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=pipe,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,           # 80 % train, 20 % val (of trainVal_df)\n",
    "    seed=SEED,\n",
    "    parallelism=4)            # adjust to cluster resources\n",
    "\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#  7.  Fit grid & choose best model\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "tvs_model = tvs.fit(train_val_df)\n",
    "best_model = tvs_model.bestModel\n",
    "best_params = {k.name: v for k, v in best_model.stages[-1]  # OvR â LinearSVCModel\n",
    "                                       .getClassifier()\n",
    "                                       .extractParamMap().items()}\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "#  8.  Final evaluation on the untouched test set\n",
    "# âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
    "pred_test = best_model.transform(test_df)\n",
    "f1_test = evaluator.evaluate(pred_test)\n",
    "print(f\"Test F1 (macro): {f1_test:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
