{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f400a1-9cf2-4303-9cea-6cf1d2806180",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8fefa-9727-4d8c-84e6-95afc8e77f27",
   "metadata": {},
   "source": [
    "In the first step we will set up the Spark environment and load the data. WE remove empty entries and load the stopwords we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a73033a-7a16-47d5-82bf-1c3a0a551557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/05/13 15:41:06 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/05/13 15:41:11 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"A2-Part2-Pipeline\").getOrCreate()\n",
    "\n",
    "#load reviews and stopwords\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    RAW_PATH = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "else:\n",
    "    RAW_PATH = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "stopwordsPath = \"Exercise_1/stopwords.txt\"\n",
    "\n",
    "#define structure of json for faster reading\n",
    "from pyspark.sql import types as T\n",
    "review_schema = T.StructType([\n",
    "     T.StructField(\"reviewerID\",      T.StringType(),  True),\n",
    "     T.StructField(\"asin\",            T.StringType(),  True),\n",
    "     T.StructField(\"reviewerName\",    T.StringType(),  True),\n",
    "     T.StructField(\"helpful\",         T.ArrayType(T.IntegerType()), True),\n",
    "     T.StructField(\"reviewText\",      T.StringType(),  True),\n",
    "     T.StructField(\"overall\",         T.FloatType(),   True),\n",
    "     T.StructField(\"summary\",         T.StringType(),  True),\n",
    "     T.StructField(\"unixReviewTime\",  T.LongType(),    True),\n",
    "     T.StructField(\"reviewTime\",      T.StringType(),  True),\n",
    "     T.StructField(\"category\",        T.StringType(),  True),\n",
    " ])\n",
    "\n",
    "#read and select category and review\n",
    "df = (\n",
    "    spark.read\n",
    "         .schema(review_schema)\n",
    "         .json(RAW_PATH)\n",
    "         .selectExpr(\"reviewText AS text\",\n",
    "                     \"category\")\n",
    "    .na.drop(subset=[\"text\", \"category\"])\n",
    ")\n",
    "\n",
    "# reading the stopwords\n",
    "stopwords = spark.sparkContext.textFile(stopwordsPath).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54af8e1-0dd3-4ae0-a6ea-b620812dafaf",
   "metadata": {},
   "source": [
    "Here, we are creating our text processing pipeline that will transform reviews into numerical features so that we can use it as input for our Machine Learning model.\n",
    "First, reviews are getting tokenized and common stopwords are getting removed.Further, we count word frequencies and convert them to TF-IDF. We also select the top 2000 most relevant terms using chi-sqaured tests.\n",
    "We also convert product categories from text to numerical labels and save that  in our selected features df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03933214-ef07-42a7-a80e-0759ec54b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer,\n",
    "    StopWordsRemover,\n",
    "    CountVectorizer,\n",
    "    IDF,\n",
    "    ChiSqSelector,\n",
    "    StringIndexer\n",
    ")\n",
    "\n",
    "# tokenisation and lower casing with RegexTokenizer\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"text\",\n",
    "    outputCol=\"tokens\",\n",
    "    pattern=r\"\"\"[ \\t0-9()\\[\\]{}.!?,;:+=\\-_\"'`~#@&*%€$§\\\\/]+\"\"\",\n",
    "    gaps=True,\n",
    "    toLowercase=True,\n",
    ")\n",
    "\n",
    "# stopword removal\n",
    "stopper = StopWordsRemover(inputCol=\"tokens\",stopWords = stopwords, outputCol=\"clean_tokens\")\n",
    "\n",
    "\n",
    "# vectorizing\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"clean_tokens\",\n",
    "    outputCol=\"tf\",\n",
    "    minDF=2,\n",
    "    vocabSize=50_000, \n",
    ")\n",
    "\n",
    "# idf weighting\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "\n",
    "# encode the category column from string to int\n",
    "encoder = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "# select top 2000 terms by chi squared\n",
    "selector = ChiSqSelector(\n",
    "    numTopFeatures=2000,\n",
    "    featuresCol=\"tfidf\",\n",
    "    outputCol=\"chi2_features\",\n",
    "    labelCol=\"label\",\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopper, cv, idf, encoder, selector])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594a4d69-244a-4402-b584-c5c7f00213e3",
   "metadata": {},
   "source": [
    "Here, we are running the whole text transformation pipeline to convert reviews into features that we can use for Machine Learning. After we have fitted, we receive the vocabulary from the CountVectorizer and also the indices from the top 2000 features from ChiSqSelector. Finally, it gets mapped back to the actual words so that we receive the final list of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63afc1c-cc8c-4d1b-a256-f9fc003c8127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 15:42:34 WARN DAGScheduler: Broadcasting large task binary with size 1243.4 KiB\n",
      "25/05/13 15:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1245.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 15:42:46 WARN DAGScheduler: Broadcasting large task binary with size 1247.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# fitting pipeline\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"128\")\n",
    "df.persist()\n",
    "model = pipeline.fit(df)\n",
    "df.unpersist()\n",
    "\n",
    "# extracting vocabulary and selected indices\n",
    "vocab = model.stages[2].vocabulary  \n",
    "selected = model.stages[-1].selectedFeatures\n",
    "\n",
    "selected_terms = [vocab[i] for i in selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c442d23-66d0-438b-b187-297331c4e35b",
   "metadata": {},
   "source": [
    "Here we are creating the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e30e7ae-6cac-4a34-be9b-093766dbd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2000 terms to /home/e12427512/Exercise_2/src/output_ds.txt\n"
     ]
    }
   ],
   "source": [
    "# saving top 2000 terms to output_ds.txt\n",
    "import pathlib, os, codecs\n",
    "out_file = pathlib.Path(\"output_ds.txt\")\n",
    "out_file.write_text(\"\\n\".join(selected_terms), encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(selected_terms)} terms to {out_file.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342d665-053d-403b-89ae-89d97bfa969d",
   "metadata": {},
   "source": [
    "Here, we are comparing the important terms from Assignment 1 with the important words we have received above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d159b-bed8-4bc7-b7ee-a7902483fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting last line of line\n",
    "lines = pathlib.Path(\"../output_dev.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "merged_vocab_line = lines[-1]  # This is the merged vocab line\n",
    "\n",
    "# splitting merged vocabs in terms\n",
    "old_terms = merged_vocab_line.strip().split()\n",
    "old_set = set(old_terms)\n",
    "\n",
    "# loading selected terms from output_ds.txt which is spark piepline\n",
    "new_terms = pathlib.Path(\"output_ds.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "new_set = set(term.strip() for term in new_terms)\n",
    "\n",
    "# comparing sets\n",
    "common_terms = old_set & new_set\n",
    "only_in_old = old_set - new_set\n",
    "only_in_new = new_set - old_set\n",
    "\n",
    "print(f\"Terms in BOTH assignments: {len(common_terms)}\")\n",
    "print(f\"Terms ONLY in Assignment 1: {len(only_in_old)}\")\n",
    "print(f\"Terms ONLY in Spark pipeline (Assignment 2): {len(only_in_new)}\")\n",
    "print(f\"Overlap: {len(common_terms) / len(new_set) * 100:.2f}% of Spark-selected terms are also in Assignment 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a22937-d901-4d24-80e1-bd48d09ea236",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2bdb31-22bc-444e-9332-4f71ac1984b8",
   "metadata": {},
   "source": [
    "For part 3, we will reuse the fitted model from part 2 for transforming our data. We will train a model that can predict the product category from a reviews text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f7bd03-d489-4209-853b-bf27a441d887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, chi2_features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reuse the fitted model from Part 2 to transform data\n",
    "transformed_df = model.transform(df).select(\"label\", \"chi2_features\")\n",
    "display(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b4f073-a26d-4413-8c2e-66f29451a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")   # WARN-Meldungen verschwinden lassen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4780d-1fb1-4693-8b48-8457bc7cfd76",
   "metadata": {},
   "source": [
    "Here, we will extend the pipeline by adding Normalization and a Support Vector Machine classifier. WE will use *Normalizer* with L2 norm.\n",
    "Because *SVM* is a binary classifier, we will use the one vs rest strategy to handle multiple categories from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5697adc-aa95-4ad0-9195-473f311bdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding NOrmalizer with L2 norm\n",
    "normalizer = Normalizer(\n",
    "    inputCol=\"chi2_features\", \n",
    "    outputCol=\"scaled_features\", \n",
    "    p=2.0\n",
    ")\n",
    "\n",
    "# adding svm as binary classifier\n",
    "binary_svm = LinearSVC(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=50,\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "# used for multi class classification\n",
    "ovr = OneVsRest(\n",
    "    classifier=binary_svm,\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n",
    "# pipeline from part 2 with added normalization and binary svm\n",
    "full_pipeline = Pipeline(stages=[\n",
    "    normalizer,\n",
    "    ovr\n",
    "])\n",
    "\n",
    "# grid search for parameter optimization\n",
    "param_grid = (ParamGridBuilder()\n",
    "    .addGrid(binary_svm.regParam, [0.01, 0.1, 1.0])\n",
    "    .addGrid(binary_svm.standardization, [True, False])\n",
    "    .addGrid(binary_svm.maxIter, [10, 50])\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481059e-4c3d-4cab-af33-987f84c4ed10",
   "metadata": {},
   "source": [
    "We will use a 60% training / 20% validation/ 20% testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88231142-e17a-42d9-9fe6-a48eb7476f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 16:53:11 ERROR BlockManagerStorageEndpoint: Error in removing broadcast 47474\n",
      "org.apache.spark.SparkException: Block broadcast_47474 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:234)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:237)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:503)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2007)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1973)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:1959)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:1959)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1959)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 18:32:30 ERROR ContextCleaner: Error cleaning broadcast 162478\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:209)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:351)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:45)\n",
      "\tat org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:79)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:256)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:204)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "Caused by: org.apache.spark.SparkException: Block broadcast_162478 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:234)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:237)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:503)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2007)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1973)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:1959)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:1959)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1959)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87072:>                                                      (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model F1 Score: 0.5975\n",
      "Best regParam: 0.01\n",
      "Best standardization: True\n",
      "Best maxIter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# we will split the data in 60% training, 20% validation and 20% testing data.\n",
    "train, val, test = transformed_df.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "\n",
    "# F1 as measure criterion\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# using cross validation to find best parameters via 3 fold CV\n",
    "cv = CrossValidator(\n",
    "    estimator=full_pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# trainin\n",
    "cv_model = cv.fit(train)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = best_model.transform(test)\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(f\"Best Model F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "# printing best parameters\n",
    "best_svm = best_model.stages[-1].getClassifier()\n",
    "print(f\"best regParam: {best_svm.getRegParam()}\")\n",
    "print(f\"best standardization: {best_svm.getStandardization()}\")\n",
    "print(f\"best maxIter: {best_svm.getMaxIter()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9f07c-47e6-4651-ae04-7b6ccddbec77",
   "metadata": {},
   "source": [
    "In the following step, we will try a reduced feature set with a new ChiSquareSelector with 500 terms, which is a much heavier filtering than with 2000 terms. In the end, we will compare the results for both, actual filtering and heavy filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd8db3-03d4-418c-aaf0-4e8c74157403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# new pipeline with heavy feature selection and 500 terms\n",
    "heavy_selector = ChiSqSelector(\n",
    "    numTopFeatures=500,\n",
    "    featuresCol=\"tfidf\",\n",
    "    outputCol=\"chi2_features_heavy\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n",
    "# new normalizer for heavy features\n",
    "normalizer_heavy = Normalizer(\n",
    "    inputCol=\"chi2_features_heavy\",\n",
    "    outputCol=\"scaled_features_heavy\",\n",
    "    p=2.0\n",
    ")\n",
    "\n",
    "# new ovr heavy features\n",
    "ovr_heavy = OneVsRest(\n",
    "    classifier=binary_svm,\n",
    "    featuresCol=\"scaled_features_heavy\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n",
    "# fitting the heavy selector on part 2 output\n",
    "heavy_model = heavy_selector.fit(model.transform(df))\n",
    "heavy_transformed = heavy_model.transform(model.transform(df)).select(\"label\", \"chi2_features_heavy\")\n",
    "\n",
    "heavy_pipeline = Pipeline(stages=[normalizer_heavy, ovr_heavy])\n",
    "\n",
    "cv_heavy = CrossValidator(\n",
    "    estimator=heavy_pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "# training and evaluate\n",
    "train_data = heavy_transformed.randomSplit([0.6, 0.2, 0.2], seed=42)[0]\n",
    "cv_heavy_model = cv_heavy.fit(train_data)\n",
    "heavy_predictions = cv_heavy_model.transform(test)\n",
    "print(f\"F1 Score (Heavy Filtering): {evaluator.evaluate(heavy_predictions):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
